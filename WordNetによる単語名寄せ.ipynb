{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####WordNetによる単語名寄せ処理####\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy.matlib\n",
    "import itertools\n",
    "import scipy\n",
    "from datetime import time, datetime, timedelta\n",
    "from scipy import sparse\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from numpy.random import *\n",
    "from scipy.special import psi \n",
    "import re\n",
    "import MeCab\n",
    "import neologdn\n",
    "import sys\n",
    "import sqlite3\n",
    "\n",
    "np.random.seed(98537)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##データの読み込み\n",
    "def data_input(iep, flag1, remove_item, remove_element):\n",
    "    ##データの設定\n",
    "    #ニュースデータの読み込み\n",
    "    input_path = \"C:/statistics/data/DJ_news_data/custom_data/DJ_fulldata_new.csv\"\n",
    "    read_data = pd.read_csv(input_path, index_col=0) \n",
    "    \n",
    "    #iep要因のみ抽出\n",
    "    if iep==1:\n",
    "        index_iep = np.array(np.where(read_data[\"model\"]==\"IEP\")[0], dtype=\"int\")\n",
    "        read_data = read_data.iloc[index_iep]\n",
    "        read_data.index = np.arange(read_data.shape[0])\n",
    "\n",
    "    #広範に影響を与える要因のレコードのみを除去\n",
    "    if flag1==1:\n",
    "        index_item = np.array(~np.in1d(read_data[\"item\"], remove_item), dtype=\"int\")\n",
    "        index_element = np.array(~np.in1d(read_data[\"element\"], remove_element), dtype=\"int\")\n",
    "        index = np.array(np.where((index_item+index_element)==2)[0], dtype=\"int\")\n",
    "        a = np.unique(read_data[\"key\"].iloc[np.delete(np.arange(read_data.shape[0]), index)])\n",
    "        b = np.unique(read_data[\"key\"].iloc[index])\n",
    "        ab = pd.merge(pd.DataFrame({\"key\": a, \"no1\": np.arange(a.shape[0])}), \n",
    "                      pd.DataFrame({\"key\": b, \"no2\": np.arange(b.shape[0])}), on=\"key\", how=\"left\")\n",
    "        remove_key = np.unique(ab[\"key\"].iloc[np.where(pd.isna(ab[\"no2\"]))[0]])\n",
    "        ab = pd.merge(read_data[[\"key\"]], pd.DataFrame({\"key\": remove_key, \"no\": np.arange(remove_key.shape[0])}), on=\"key\", how=\"left\")\n",
    "        read_data = read_data.iloc[np.where(pd.isna(ab[\"no\"]))[0]]\n",
    "        read_data.index = np.arange(read_data.shape[0])\n",
    "    #広範に影響を与える要因を含むニュースを除去\n",
    "    if flag1==2:\n",
    "        index_item = np.array(np.in1d(read_data[\"item\"], remove_item), dtype=\"int\")\n",
    "        index_element = np.array(np.in1d(read_data[\"element\"], remove_element), dtype=\"int\")   \n",
    "        index = np.array(np.where((index_item+index_element) > 0)[0], dtype=\"int\")\n",
    "        key = np.unique(read_data[\"key\"].iloc[index])\n",
    "        delete_key = pd.merge(read_data[[\"key\"]], pd.DataFrame({\"key\": key, \"no\": np.arange(key.shape[0])}), on=\"key\", how=\"left\")\n",
    "        read_data = read_data.iloc[np.where(pd.isna(delete_key[\"no\"])==True)]\n",
    "        read_data.index = np.arange(read_data.shape[0])\n",
    "        \n",
    "    #カラムの入れ替えとインデックスの定義\n",
    "    read_data = read_data[[\"key\", \"date\", \"headline\", \"text\", \"area\", \"subject\", \"item\", \"element\", \"predicate\", \"trend\",\n",
    "                           \"tags\", \"complete\", \"model\", \"aiep\", \"identified\"]]\n",
    "    read_data.index = np.arange(read_data.shape[0])\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##データの前処理\n",
    "def data_preprocess(read_data):\n",
    "    ##分析済みのデータのみを取り出す\n",
    "    index_get = np.array(np.where(np.array(pd.isna(read_data[[\"aiep\"]])==False).reshape(-1))[0], dtype=\"int\")\n",
    "    df = read_data.iloc[index_get, ]\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    del read_data\n",
    "    \n",
    "    ##単語の名寄せを行う\n",
    "    #データの読み込み\n",
    "    area_dic = pd.read_csv(\"C:/statistics/data/dic/area_pattern_freq.csv\", encoding=\"Shift-Jis\")\n",
    "    item_dic = pd.read_csv(\"C:/statistics/data/dic/item_pattern_freq.csv\", encoding=\"Shift-Jis\")\n",
    "\n",
    "    #辞書から単語を名寄せ\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df = pd.merge(tmp_df, area_dic, left_on=\"area\", right_on=\"input\", how=\"left\")\n",
    "    tmp_df = pd.merge(tmp_df, item_dic, left_on=\"item\", right_on=\"input\", how=\"left\")\n",
    "    df[\"area\"] = tmp_df[\"output2\"]; df[\"item\"] = tmp_df[\"output\"]\n",
    "    del tmp_df\n",
    "    \n",
    "    #要因がエリア以外1つしか観測されていないニュースを除く\n",
    "    df = df.iloc[np.where(np.sum(np.array(~pd.isna(df[[\"item\", \"element\", \"subject\", \"trend\"]])), axis=1) > 1)[0]]\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    \n",
    "    #aiepがすべてnanのニュースを取り除く\n",
    "    Z = np.zeros((df.shape[0], 5), dtype=\"int\")\n",
    "    Z[:, 0] = ~pd.isna(df[\"area\"])\n",
    "    Z[:, 1] = ~pd.isna(df[\"item\"])\n",
    "    Z[:, 2] = ~pd.isna(df[\"element\"])\n",
    "    Z[:, 3] = ~pd.isna(df[\"subject\"])\n",
    "    Z[:, 4] = ~pd.isna(df[\"trend\"])\n",
    "    df = df.iloc[np.where(np.sum(Z, axis=1) >= 2)[0]]\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    \n",
    "    ##データの設定\n",
    "    #日付をdatetime型に変更\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"].str[0:19])\n",
    "    date_range = np.array([np.min(df[\"date\"][df[\"date\"] > \"2010\"]), np.max(df[\"date\"])])\n",
    "    #date_range = np.array([np.min(panel_data[\"日付\"]), np.max(panel_data[\"日付\"])])\n",
    "\n",
    "    #ニュースのある期間のデータのみ抽出\n",
    "    index = np.array(np.where((df[\"date\"] > date_range[0]) & (df[\"date\"] <= date_range[1]))[0], dtype=\"int\")\n",
    "    target_df = df.iloc[index]\n",
    "    target_df.index = np.arange(target_df.shape[0])\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aiepの組み合わせの個数をカウントする\n",
    "def pattern_count():\n",
    "    area_vec = target_df[\"area\"]; area_vec[pd.isna(area_vec)] = \"抽出なし\"\n",
    "    item_vec = target_df[\"item\"]; item_vec[pd.isna(item_vec)] = \"抽出なし\"\n",
    "    element_vec = target_df[\"element\"]; element_vec[pd.isna(element_vec)] = \"抽出なし\"\n",
    "    subject_vec = target_df[\"subject\"]; subject_vec[pd.isna(subject_vec)] = \"抽出なし\"\n",
    "    trend_vec = target_df[\"trend\"]; trend_vec[pd.isna(trend_vec)] = \"抽出なし\"\n",
    "    aiep_vec = area_vec + \" - \" + item_vec + \" - \" + element_vec + \" - \" + subject_vec + \" - \" + trend_vec\n",
    "    res = aiep_vec.value_counts()\n",
    "    freq_df = pd.DataFrame({\"pattern\": np.array(res.index), \"freq\": np.array(res, dtype=\"int\")})\n",
    "    freq_df.to_csv(\"C:/statistics/data/aiep_pattern_freq.csv\", sep=\",\")\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ニュースソースを削減する\n",
    "def delete_news(target_df):\n",
    "    \n",
    "    #要因の個数を集計する\n",
    "    area_count = pd.Series.value_counts(target_df[\"area\"])\n",
    "    item_count = pd.Series.value_counts(target_df[\"item\"])\n",
    "    subject_count = pd.Series.value_counts(target_df[\"subject\"])\n",
    "    element_count = pd.Series.value_counts(target_df[\"element\"])\n",
    "\n",
    "    area_count.to_csv(\"C:/statistics/data/area_pattern_freq.csv\", sep=\",\")\n",
    "    item_count.to_csv(\"C:/statistics/data/item_pattern_freq.csv\", sep=\",\")\n",
    "    subject_count.to_csv(\"C:/statistics/data/subject_pattern_freq.csv\", sep=\",\")\n",
    "    element_count.to_csv(\"C:/statistics/data/element_pattern_freq.csv\", sep=\",\")\n",
    "    \n",
    "    ##aiepに数値idを設定\n",
    "    #ユニークな要素を抽出\n",
    "    unique_area = pd.unique(target_df[\"area\"]); area_n = unique_area.shape[0]\n",
    "    unique_item = pd.unique(target_df[\"item\"]); item_n = unique_item.shape[0]\n",
    "    unique_subject = pd.unique(target_df[\"subject\"]); subject_n = unique_subject.shape[0]\n",
    "    unique_element = pd.unique(target_df[\"element\"]); element_n = unique_element.shape[0]\n",
    "    unique_trend = pd.unique(target_df[\"trend\"]); trend_n = unique_trend.shape[0]\n",
    "    unique_predicate = pd.unique(target_df[\"predicate\"]); predicate_n = unique_predicate.shape[0]\n",
    "    unique_tags = pd.unique(target_df[\"tags\"]); tags_n = unique_tags.shape[0]\n",
    "\n",
    "    #マスターデータにidを設定\n",
    "    area_df = pd.DataFrame({\"area\": unique_area, \"id\": np.arange(area_n)})\n",
    "    area_id = np.array(pd.merge(target_df[[\"area\"]], area_df, on=\"area\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_item = np.append(unique_item[~pd.isna(pd.Series(unique_item))], np.nan)\n",
    "    item_df = pd.DataFrame({\"item\": unique_item, \"id\": np.arange(item_n)})\n",
    "    item_id = np.array(pd.merge(target_df[[\"item\"]], item_df, on=\"item\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_subject = np.append(unique_subject[~pd.isna(pd.Series(unique_subject))], np.nan)\n",
    "    subject_df = pd.DataFrame({\"subject\": unique_subject, \"id\": np.arange(subject_n)})\n",
    "    subject_id = np.array(pd.merge(target_df[[\"subject\"]], subject_df, on=\"subject\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_element = np.append(unique_element[~pd.isna(pd.Series(unique_element))], np.nan)\n",
    "    element_df = pd.DataFrame({\"element\": unique_element, \"id\": np.arange(element_n)})\n",
    "    element_id = np.array(pd.merge(target_df[[\"element\"]], element_df, on=\"element\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_trend = np.append(unique_trend[~pd.isna(pd.Series(unique_trend))], np.nan)\n",
    "    trend_df = pd.DataFrame({\"trend\": unique_trend, \"id\": np.arange(trend_n)})\n",
    "    trend_id = np.array(pd.merge(target_df[[\"trend\"]], trend_df, on=\"trend\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "    return target_df, area_df, area_id, item_df, item_id, subject_df, subject_id, element_df, element_id, trend_df, trend_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ニュースソースの重複を削除する\n",
    "def correspond_data(target_df, area_id, item_id, subject_id, element_id, trend_id):\n",
    "    #ニュースデータの日付を市場が開いている時間に繰り越す\n",
    "    index = np.array(np.where((target_df[\"date\"].apply(lambda x:x.time()) >= time(hour=0)) & \n",
    "                              (target_df[\"date\"].apply(lambda x:x.time()) <= time(hour=15)))[0], dtype=\"int\")\n",
    "    index_target = np.delete(np.arange(target_df.shape[0]), index)\n",
    "    new_date = target_df[[\"date\"]].copy()\n",
    "    new_date[\"date\"].iloc[index_target] = target_df[\"date\"].iloc[index_target] + timedelta(days=1)\n",
    "\n",
    "    #日付のデータ型を数値型に変更\n",
    "    df_date = np.array((new_date[\"date\"].dt.date.astype(\"str\")).str.replace(\"-\", \"\"), dtype=\"int\")\n",
    "    unique_date = np.array(np.sort(np.unique(df_date)), dtype=\"int\")  \n",
    "    date_n = unique_date.shape[0]\n",
    "\n",
    "    #重複しているニュースを特定\n",
    "    tmp_df = pd.concat((pd.DataFrame(df_date), target_df[[\"area\", \"subject\", \"item\", \"element\", \"trend\"]]), axis=1)\n",
    "    tmp_df = tmp_df.rename(columns={0: \"date\"})\n",
    "    tmp_df = tmp_df.fillna(\"hoge\")\n",
    "    index_dup = np.array(tmp_df.duplicated())\n",
    "    joint_tag = tmp_df[\"date\"].astype(\"U8\") + \"-\" + tmp_df[\"area\"] + \"-\" + tmp_df[\"subject\"] +\\\n",
    "                    \"- \" + tmp_df[\"item\"] + \"-\" + tmp_df[\"element\"] + \"-\" + tmp_df[\"trend\"]\n",
    "    joint_count = joint_tag.value_counts()\n",
    "    pd.DataFrame({\"tag\": joint_count.index, \"freq\": np.array(joint_count, dtype=\"int\")}).to_csv(\"C:/statistics/data/record_dup.csv\")\n",
    "\n",
    "    #重複を削除\n",
    "    target_df = target_df.iloc[~index_dup, ]\n",
    "    target_df.index = np.arange(target_df.shape[0])\n",
    "    area_id = area_id[~index_dup]\n",
    "    item_id = item_id[~index_dup]\n",
    "    subject_id = subject_id[~index_dup]\n",
    "    element_id = element_id[~index_dup]\n",
    "    trend_id = trend_id[~index_dup]\n",
    "    df_date = df_date[~index_dup]\n",
    "    \n",
    "    return target_df, area_id, item_id, subject_id, element_id, trend_id, df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##インデックスを設定\n",
    "def create_index(area_id, item_id, subject_id, element_id, trend_id):\n",
    "    #アイテムごとのユニーク数を数える\n",
    "    area_n = np.unique(area_id).shape[0]\n",
    "    item_n = np.unique(item_id).shape[0]\n",
    "    subject_n = np.unique(subject_id).shape[0]\n",
    "    element_n = np.unique(element_id).shape[0]\n",
    "    trend_n = np.unique(trend_id).shape[0]\n",
    "    \n",
    "    #インデックスを定義\n",
    "    index_area = [i for i in range(area_n)]\n",
    "    index_item = [i for i in range(item_n)]\n",
    "    index_subject = [i for i in range(subject_n)]\n",
    "    index_element = [i for i in range(element_n)]\n",
    "    index_trend = [i for i in range(trend_n)]\n",
    "    for i in range(area_n):\n",
    "        index_area[i] = np.array(np.where(area_id==i)[0], dtype=\"int\")\n",
    "    for i in range(item_n):\n",
    "        index_item[i] = np.array(np.where(item_id==i)[0], dtype=\"int\")\n",
    "    for i in range(subject_n):\n",
    "        index_subject[i] = np.array(np.where(subject_id==i)[0], dtype=\"int\")\n",
    "    for i in range(element_n):\n",
    "        index_element[i] = np.array(np.where(element_id==i)[0], dtype=\"int\")\n",
    "    for i in range(trend_n):\n",
    "        index_trend[i] = np.array(np.where(trend_id==i)[0], dtype=\"int\")\n",
    "    return index_area, index_item, index_subject, index_element, index_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##文書をテキストとaiepに分離する\n",
    "def df_allocation(target_df):\n",
    "    #ユニークなテキストを抽出\n",
    "    index = np.array(np.where(~target_df[\"key\"].duplicated()==True)[0], dtype=\"int\")\n",
    "    key_id = pd.DataFrame(np.arange(index.shape[0])[:, np.newaxis]).rename(columns={0: \"key_id\"})\n",
    "    text_data = target_df[[\"key\", \"date\", \"headline\", \"text\"]].iloc[index]\n",
    "    text_data.index = np.arange(index.shape[0])\n",
    "    text_data = pd.concat((key_id, text_data), axis=1)\n",
    "\n",
    "    #aiepのデータフレームを作成\n",
    "    aiep_data = target_df[[\"key\", \"date\", \"area\", \"subject\", \"item\", \"element\", \"predicate\", \"trend\", \"tags\", \"model\", \"aiep\"]]\n",
    "    temp_id = pd.merge(aiep_data[[\"key\"]], text_data[[\"key\", \"key_id\"]], on=\"key\", how=\"left\")[[\"key_id\"]]\n",
    "    aiep_data = pd.concat((temp_id, aiep_data), axis=1)\n",
    "    return text_data, aiep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##パラグラフ単位のテキストを結合\n",
    "def paragraph_text(text_data):\n",
    "    #データの読み込み\n",
    "    pf2010 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2010.csv\")\n",
    "    pf2010 = pf2010[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2011 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2011.csv\")\n",
    "    pf2011 = pf2011[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2012 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2012.csv\")\n",
    "    pf2012 = pf2012[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2013 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2013.csv\")\n",
    "    pf2013 = pf2013[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2014 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2014.csv\")\n",
    "    pf2014 = pf2014[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2015 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2015.csv\")\n",
    "    pf2015 = pf2015[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2016 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2016.csv\")\n",
    "    pf2016 = pf2016[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2017 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2017.csv\")\n",
    "    pf2017 = pf2017[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2018 = pd.read_csv(\"C:/statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2018.csv\")\n",
    "    pf2018 = pf2018[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf = pd.concat((pf2010, pf2011, pf2012, pf2013, pf2014, pf2015, pf2016, pf2017, pf2018), axis=0)\n",
    "    pf.index = np.arange(pf.shape[0])\n",
    "\n",
    "    #ターゲットのテキストを抽出\n",
    "    key_id = np.array(pd.merge(pf[[\"key\"]], text_data[[\"key\", \"key_id\"]], on=\"key\", how=\"left\")[[\"key_id\"]]).reshape(-1)\n",
    "    index_target = np.array(np.where(np.isnan(key_id)==False)[0], dtype=\"int\")\n",
    "    target_pf = pf.iloc[index_target]\n",
    "    target_pf[\"key_id\"] = np.array(key_id[index_target], dtype=\"int\")\n",
    "    target_pf = target_pf[[\"key_id\", \"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    target_pf.index = np.arange(target_pf.shape[0])\n",
    "    return target_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定の単語を入力とした時に、類義語を検索する関数\n",
    "def SearchSimilarWords(word):\n",
    "\n",
    "    # 問い合わせしたい単語がWordnetに存在するか確認する\n",
    "    cur = conn.execute(\"select wordid from word where lemma='%s'\" % word)\n",
    "    word_id = 99999999  #temp \n",
    "    for row in cur:\n",
    "        word_id = row[0]\n",
    "\n",
    "    # Wordnetに存在する語であるかの判定\n",
    "    if word_id==99999999:\n",
    "        print(\"「%s」は、Wordnetに存在しない単語です。\" % word)\n",
    "        return \n",
    "    else:\n",
    "        print(\"【「%s」の類似語を出力します】\\n\" % word)\n",
    "\n",
    "    # 入力された単語を含む概念を検索する\n",
    "    cur = conn.execute(\"select synset from sense where wordid='%s'\" % word_id)\n",
    "    synsets = []\n",
    "    for row in cur:\n",
    "        synsets.append(row[0])\n",
    "\n",
    "    # 概念に含まれる単語を検索して画面出力する\n",
    "    no = 0\n",
    "    conception = [\"\" for j in range(len(synsets))]\n",
    "    meaning = [\"\" for j in range(len(synsets))]\n",
    "    synonym = [\"\" for j in range(len(synsets))]\n",
    "    for synset in synsets:\n",
    "        cur1 = conn.execute(\"select name from synset where synset='%s'\" % synset)\n",
    "        conception0 = np.array([])\n",
    "        for row1 in cur1:\n",
    "            conception0 = np.append(conception0, np.array(row1[0]))\n",
    "        cur2 = conn.execute(\"select def from synset_def where (synset='%s' and lang='jpn')\" % synset)\n",
    "        meaning0 = np.array([])\n",
    "        for row2 in cur2:\n",
    "            meaning0 = np.append(meaning0, np.array(row2[0]))\n",
    "        cur3 = conn.execute(\"select wordid from sense where (synset='%s' and wordid!=%s)\" % (synset,word_id))\n",
    "        synonym0 = np.array([])\n",
    "        for row3 in cur3:\n",
    "            target_word_id = row3[0]\n",
    "            cur3_1 = conn.execute(\"select lemma from word where wordid=%s\" % target_word_id)\n",
    "            for row3_1 in cur3_1:\n",
    "                synonym0 = np.append(synonym0, np.array(row3_1[0]))\n",
    "        conception[no] = np.array(conception0, dtype=\"object\")\n",
    "        meaning[no] = np.array(meaning0, dtype=\"object\")\n",
    "        synonym[no] = np.array(np.sort(np.append(synonym0, word)), dtype=\"object\")\n",
    "        no += 1\n",
    "    return conception, meaning, synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\statistics\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3185: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n",
      "C:\\statistics\\anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "##データ解析対象のテキストの抽出とクレンジング\n",
    "#データの読み込み\n",
    "remove_item = np.array([\"原油\", \"石油\", \"米国債\", \"米ドル\", \"ユーロ\", \"日本円\", \"日本国債\", \"株式\"])\n",
    "remove_element = np.array([\"政策金利\", \"金利\", \"株式市場\", \"米国株\", \"株価\", \"利回り\", \"経済\"])\n",
    "read_data = data_input(1, 1, remove_item, remove_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\statistics\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#データの加工とデータの絞り込み\n",
    "target_df = data_preprocess(read_data)\n",
    "res_delete_news = delete_news(target_df)\n",
    "target_df, area_df, area_id, item_df, item_id, subject_df, subject_id, element_df, element_id, trend_df, trend_id = res_delete_news\n",
    "del res_delete_news\n",
    "\n",
    "#テキスト本文とaiepをそれぞれデータフレーム化する\n",
    "res_correspond_data = correspond_data(target_df, area_id, item_id, subject_id, element_id, trend_id)\n",
    "target_df, area_id, item_id, subject_id, element_id, trend_id, df_date = res_correspond_data\n",
    "del res_correspond_data\n",
    "index_area, index_item, index_subject, index_element, index_trend = create_index(area_id, item_id, subject_id, element_id, trend_id)\n",
    "text_data, aiep_data = df_allocation(target_df)\n",
    "copy_data1 = text_data.copy(); copy_data = aiep_data.copy()   #バックアップ\n",
    "target_pf = paragraph_text(text_data)   #パラグラフ単位のテキストと結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#単語の名寄せ処理\n",
    "conn = sqlite3.connect(\"C:/statistics/data/dic/wnjpn.db\")   #データベースに接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_item = pd.unique(aiep_data[\"item\"].iloc[np.where(~pd.isna(aiep_data[\"item\"]))[0]])\n",
    "unique_element = pd.unique(aiep_data[\"element\"].iloc[np.where(~pd.isna(aiep_data[\"element\"]))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 問い合わせしたい単語がWordnetに存在するか確認する\n",
    "no_list = [i for i in range(unique_item.shape[0])]\n",
    "word_list = [i for i in range(unique_item.shape[0])]\n",
    "synonym_list = [i for i in range(unique_item.shape[0])]\n",
    "for i in range(unique_item.shape[0]):\n",
    "    out = SearchSimilarWords(unique_item[i])\n",
    "    if len(out[0])==0:\n",
    "        no_list[i] = np.repeat(i, 1)\n",
    "        word_list[i] = np.array([unique_item[i]], dtype=\"object\")\n",
    "        synonym_list[i] = np.array([\"Wordnetには存在しません\"])\n",
    "        continue\n",
    "    word = np.array(np.repeat(\"\", len(out[2])), dtype=\"object\")\n",
    "    synonym = np.array(np.repeat(\"\", len(out[2])), dtype=\"object\")\n",
    "    for j in range(len(out[2])):\n",
    "        word[j] = unique_item[i]\n",
    "        q = out[2][j]\n",
    "        synonym[j] = \" - \".join([str(n) for n in q])\n",
    "    no_list[i] = np.repeat(i, word.shape[0])\n",
    "    word_list[i] = word\n",
    "    synonym_list[i] = synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 類義語のパターンを出力\n",
    "no = np.array(list(itertools.chain(*[no_list[i] for i in range(unique_item.shape[0])])))\n",
    "word = np.array(list(itertools.chain(*[word_list[i] for i in range(unique_item.shape[0])])), dtype=\"object\")\n",
    "synonym = np.array(list(itertools.chain(*[synonym_list[i] for i in range(unique_item.shape[0])])), dtype=\"object\")\n",
    "out_data = pd.DataFrame({\"no\": no, \"word\": word, \"synonym\": synonym})\n",
    "out_data = out_data.sort_values(by=\"synonym\")\n",
    "out_data.index = np.arange(out_data.shape[0])\n",
    "temp_data = pd.DataFrame({\"synonym\": pd.unique(synonym), \"pattern\": np.arange(np.unique(synonym).shape[0])})\n",
    "out_data = pd.merge(out_data, temp_data, on=\"synonym\", how=\"left\")\n",
    "out_data = out_data[[\"no\", \"word\", \"pattern\", \"synonym\"]]\n",
    "out_data = out_data.sort_values(by=\"pattern\")\n",
    "freq = out_data[\"pattern\"].value_counts()\n",
    "freq = pd.DataFrame({\"pattern\": np.array(freq.index, dtype=\"int\"), \"freq\" :np.array(freq, dtype=\"int\")})\n",
    "out_data = pd.merge(out_data, freq, on=\"pattern\", how=\"left\")\n",
    "out_data.to_csv(\"C:/statistics/data/synonym_data.csv\", index=None, encoding=\"Shift-Jis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
