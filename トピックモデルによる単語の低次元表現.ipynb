{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####トピックモデルによる単語の低ランク表現####\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy.matlib\n",
    "import itertools\n",
    "import scipy\n",
    "from datetime import time, datetime, timedelta\n",
    "from scipy import sparse\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from numpy.random import *\n",
    "from scipy.special import psi \n",
    "import re\n",
    "import MeCab\n",
    "import neologdn\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "np.random.seed(98537)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##データの読み込み\n",
    "def data_input(iep, flag1, remove_item, remove_element):\n",
    "    ##データの設定\n",
    "    #ニュースデータの読み込み\n",
    "    input_path = \"D:/Statistics/data/DJ_news_data/custom_data/DJ_fulldata_new.csv\"\n",
    "    read_data = pd.read_csv(input_path, index_col=0) \n",
    "    \n",
    "    #iep要因のみ抽出\n",
    "    if iep==1:\n",
    "        index_iep = np.array(np.where(read_data[\"model\"]==\"IEP\")[0], dtype=\"int\")\n",
    "        read_data = read_data.iloc[index_iep]\n",
    "        read_data.index = np.arange(read_data.shape[0])\n",
    "\n",
    "    #広範に影響を与える要因のレコードのみを除去\n",
    "    if flag1==1:\n",
    "        index_item = np.array(~np.in1d(read_data[\"item\"], remove_item), dtype=\"int\")\n",
    "        index_element = np.array(~np.in1d(read_data[\"element\"], remove_element), dtype=\"int\")\n",
    "        index = np.array(np.where((index_item+index_element)==2)[0], dtype=\"int\")\n",
    "        a = np.unique(read_data[\"key\"].iloc[np.delete(np.arange(read_data.shape[0]), index)])\n",
    "        b = np.unique(read_data[\"key\"].iloc[index])\n",
    "        ab = pd.merge(pd.DataFrame({\"key\": a, \"no1\": np.arange(a.shape[0])}), \n",
    "                      pd.DataFrame({\"key\": b, \"no2\": np.arange(b.shape[0])}), on=\"key\", how=\"left\")\n",
    "        remove_key = np.unique(ab[\"key\"].iloc[np.where(pd.isna(ab[\"no2\"]))[0]])\n",
    "        ab = pd.merge(read_data[[\"key\"]], pd.DataFrame({\"key\": remove_key, \"no\": np.arange(remove_key.shape[0])}), on=\"key\", how=\"left\")\n",
    "        read_data = read_data.iloc[np.where(pd.isna(ab[\"no\"]))[0]]\n",
    "        read_data.index = np.arange(read_data.shape[0])\n",
    "        \n",
    "    #広範に影響を与える要因を含むニュースを除去\n",
    "    if flag1==2:\n",
    "        index_item = np.array(np.in1d(read_data[\"item\"], remove_item), dtype=\"int\")\n",
    "        index_element = np.array(np.in1d(read_data[\"element\"], remove_element), dtype=\"int\")   \n",
    "        index = np.array(np.where((index_item+index_element) > 0)[0], dtype=\"int\")\n",
    "        key = np.unique(read_data[\"key\"].iloc[index])\n",
    "        delete_key = pd.merge(read_data[[\"key\"]], pd.DataFrame({\"key\": key, \"no\": np.arange(key.shape[0])}), on=\"key\", how=\"left\")\n",
    "        read_data = read_data.iloc[np.where(pd.isna(delete_key[\"no\"])==True)]\n",
    "        read_data.index = np.arange(read_data.shape[0])\n",
    "        \n",
    "    #カラムの入れ替えとインデックスの定義\n",
    "    read_data = read_data[[\"key\", \"date\", \"headline\", \"text\", \"area\", \"subject\", \"item\", \"element\", \"predicate\", \"trend\",\n",
    "                           \"tags\", \"complete\", \"model\", \"aiep\", \"identified\"]]\n",
    "    read_data.index = np.arange(read_data.shape[0])\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##データの前処理\n",
    "def data_preprocess(read_data):\n",
    "    ##分析済みのデータのみを取り出す\n",
    "    index_get = np.array(np.where(np.array(pd.isna(read_data[[\"aiep\"]])==False).reshape(-1))[0], dtype=\"int\")\n",
    "    df = read_data.iloc[index_get, ]\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    del read_data\n",
    "    \n",
    "    ##単語の名寄せを行う\n",
    "    #データの読み込み\n",
    "    area_dic = pd.read_csv(\"D:/Statistics/data/dic/area_pattern_freq.csv\", encoding=\"Shift-Jis\")\n",
    "    item_dic = pd.read_csv(\"D:/Statistics/data/dic/item_pattern_freq.csv\", encoding=\"Shift-Jis\")\n",
    "\n",
    "    #辞書から単語を名寄せ\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df = pd.merge(tmp_df, area_dic, left_on=\"area\", right_on=\"input\", how=\"left\")\n",
    "    tmp_df = pd.merge(tmp_df, item_dic, left_on=\"item\", right_on=\"input\", how=\"left\")\n",
    "    df[\"area\"] = tmp_df[\"output2\"]; df[\"item\"] = tmp_df[\"output\"]\n",
    "    del tmp_df\n",
    "    \n",
    "    #要因がエリア以外1つしか観測されていないニュースを除く\n",
    "    df = df.iloc[np.where(np.sum(np.array(~pd.isna(df[[\"item\", \"element\", \"subject\", \"trend\"]])), axis=1) > 1)[0]]\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    \n",
    "    #aiepがすべてnanのニュースを取り除く\n",
    "    Z = np.zeros((df.shape[0], 5), dtype=\"int\")\n",
    "    Z[:, 0] = ~pd.isna(df[\"area\"])\n",
    "    Z[:, 1] = ~pd.isna(df[\"item\"])\n",
    "    Z[:, 2] = ~pd.isna(df[\"element\"])\n",
    "    Z[:, 3] = ~pd.isna(df[\"subject\"])\n",
    "    Z[:, 4] = ~pd.isna(df[\"trend\"])\n",
    "    df = df.iloc[np.where(np.sum(Z, axis=1) >= 2)[0]]\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    \n",
    "    ##データの設定\n",
    "    #日付をdatetime型に変更\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"].str[0:19])\n",
    "    date_range = np.array([np.min(df[\"date\"][df[\"date\"] > \"2010\"]), np.max(df[\"date\"])])\n",
    "    #date_range = np.array([np.min(panel_data[\"日付\"]), np.max(panel_data[\"日付\"])])\n",
    "\n",
    "    #ニュースのある期間のデータのみ抽出\n",
    "    index = np.array(np.where((df[\"date\"] > date_range[0]) & (df[\"date\"] <= date_range[1]))[0], dtype=\"int\")\n",
    "    target_df = df.iloc[index]\n",
    "    target_df.index = np.arange(target_df.shape[0])\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aiepの組み合わせの個数をカウントする\n",
    "def pattern_count():\n",
    "    area_vec = target_df[\"area\"]; area_vec[pd.isna(area_vec)] = \"抽出なし\"\n",
    "    item_vec = target_df[\"item\"]; item_vec[pd.isna(item_vec)] = \"抽出なし\"\n",
    "    element_vec = target_df[\"element\"]; element_vec[pd.isna(element_vec)] = \"抽出なし\"\n",
    "    subject_vec = target_df[\"subject\"]; subject_vec[pd.isna(subject_vec)] = \"抽出なし\"\n",
    "    trend_vec = target_df[\"trend\"]; trend_vec[pd.isna(trend_vec)] = \"抽出なし\"\n",
    "    aiep_vec = area_vec + \" - \" + item_vec + \" - \" + element_vec + \" - \" + subject_vec + \" - \" + trend_vec\n",
    "    res = aiep_vec.value_counts()\n",
    "    freq_df = pd.DataFrame({\"pattern\": np.array(res.index), \"freq\": np.array(res, dtype=\"int\")})\n",
    "    freq_df.to_csv(\"D:/Statistics/data/aiep_pattern_freq.csv\", sep=\",\")\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ニュースソースを削減する\n",
    "def delete_news(target_df):\n",
    "    \n",
    "    #要因の個数を集計する\n",
    "    area_count = pd.Series.value_counts(target_df[\"area\"])\n",
    "    item_count = pd.Series.value_counts(target_df[\"item\"])\n",
    "    subject_count = pd.Series.value_counts(target_df[\"subject\"])\n",
    "    element_count = pd.Series.value_counts(target_df[\"element\"])\n",
    "\n",
    "    area_count.to_csv(\"D:/Statistics/data/area_pattern_freq.csv\", sep=\",\")\n",
    "    item_count.to_csv(\"D:/Statistics/data/item_pattern_freq.csv\", sep=\",\")\n",
    "    subject_count.to_csv(\"D:/Statistics/data/subject_pattern_freq.csv\", sep=\",\")\n",
    "    element_count.to_csv(\"D:/Statistics/data/element_pattern_freq.csv\", sep=\",\")\n",
    "    \n",
    "    ##aiepに数値idを設定\n",
    "    #ユニークな要素を抽出\n",
    "    unique_area = pd.unique(target_df[\"area\"]); area_n = unique_area.shape[0]\n",
    "    unique_item = pd.unique(target_df[\"item\"]); item_n = unique_item.shape[0]\n",
    "    unique_subject = pd.unique(target_df[\"subject\"]); subject_n = unique_subject.shape[0]\n",
    "    unique_element = pd.unique(target_df[\"element\"]); element_n = unique_element.shape[0]\n",
    "    unique_trend = pd.unique(target_df[\"trend\"]); trend_n = unique_trend.shape[0]\n",
    "    unique_predicate = pd.unique(target_df[\"predicate\"]); predicate_n = unique_predicate.shape[0]\n",
    "    unique_tags = pd.unique(target_df[\"tags\"]); tags_n = unique_tags.shape[0]\n",
    "\n",
    "    #マスターデータにidを設定\n",
    "    area_df = pd.DataFrame({\"area\": unique_area, \"id\": np.arange(area_n)})\n",
    "    area_id = np.array(pd.merge(target_df[[\"area\"]], area_df, on=\"area\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_item = np.append(unique_item[~pd.isna(pd.Series(unique_item))], np.nan)\n",
    "    item_df = pd.DataFrame({\"item\": unique_item, \"id\": np.arange(item_n)})\n",
    "    item_id = np.array(pd.merge(target_df[[\"item\"]], item_df, on=\"item\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_subject = np.append(unique_subject[~pd.isna(pd.Series(unique_subject))], np.nan)\n",
    "    subject_df = pd.DataFrame({\"subject\": unique_subject, \"id\": np.arange(subject_n)})\n",
    "    subject_id = np.array(pd.merge(target_df[[\"subject\"]], subject_df, on=\"subject\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_element = np.append(unique_element[~pd.isna(pd.Series(unique_element))], np.nan)\n",
    "    element_df = pd.DataFrame({\"element\": unique_element, \"id\": np.arange(element_n)})\n",
    "    element_id = np.array(pd.merge(target_df[[\"element\"]], element_df, on=\"element\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "\n",
    "    unique_trend = np.append(unique_trend[~pd.isna(pd.Series(unique_trend))], np.nan)\n",
    "    trend_df = pd.DataFrame({\"trend\": unique_trend, \"id\": np.arange(trend_n)})\n",
    "    trend_id = np.array(pd.merge(target_df[[\"trend\"]], trend_df, on=\"trend\", how=\"left\")[\"id\"], dtype=\"int\")\n",
    "    return target_df, area_df, area_id, item_df, item_id, subject_df, subject_id, element_df, element_id, trend_df, trend_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ニュースソースの重複を削除する\n",
    "def correspond_data(target_df, area_id, item_id, subject_id, element_id, trend_id):\n",
    "    #ニュースデータの日付を市場が開いている時間に繰り越す\n",
    "    index = np.array(np.where((target_df[\"date\"].apply(lambda x:x.time()) >= time(hour=0)) & \n",
    "                              (target_df[\"date\"].apply(lambda x:x.time()) <= time(hour=15)))[0], dtype=\"int\")\n",
    "    index_target = np.delete(np.arange(target_df.shape[0]), index)\n",
    "    new_date = target_df[[\"date\"]].copy()\n",
    "    new_date[\"date\"].iloc[index_target] = target_df[\"date\"].iloc[index_target] + timedelta(days=1)\n",
    "\n",
    "    #日付のデータ型を数値型に変更\n",
    "    df_date = np.array((new_date[\"date\"].dt.date.astype(\"str\")).str.replace(\"-\", \"\"), dtype=\"int\")\n",
    "    unique_date = np.array(np.sort(np.unique(df_date)), dtype=\"int\")  \n",
    "    date_n = unique_date.shape[0]\n",
    "\n",
    "    #重複しているニュースを特定\n",
    "    tmp_df = pd.concat((pd.DataFrame(df_date), target_df[[\"area\", \"subject\", \"item\", \"element\", \"trend\"]]), axis=1)\n",
    "    tmp_df = tmp_df.rename(columns={0: \"date\"})\n",
    "    tmp_df = tmp_df.fillna(\"hoge\")\n",
    "    index_dup = np.array(tmp_df.duplicated())\n",
    "    joint_tag = tmp_df[\"date\"].astype(\"U8\") + \"-\" + tmp_df[\"area\"] + \"-\" + tmp_df[\"subject\"] +\\\n",
    "                    \"- \" + tmp_df[\"item\"] + \"-\" + tmp_df[\"element\"] + \"-\" + tmp_df[\"trend\"]\n",
    "    joint_count = joint_tag.value_counts()\n",
    "    pd.DataFrame({\"tag\": joint_count.index, \"freq\": np.array(joint_count, dtype=\"int\")}).to_csv(\"D:/Statistics/data/record_dup.csv\")\n",
    "\n",
    "    #重複を削除\n",
    "    target_df = target_df.iloc[~index_dup, ]\n",
    "    target_df.index = np.arange(target_df.shape[0])\n",
    "    area_id = area_id[~index_dup]\n",
    "    item_id = item_id[~index_dup]\n",
    "    subject_id = subject_id[~index_dup]\n",
    "    element_id = element_id[~index_dup]\n",
    "    trend_id = trend_id[~index_dup]\n",
    "    df_date = df_date[~index_dup]\n",
    "    \n",
    "    return target_df, area_id, item_id, subject_id, element_id, trend_id, df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##インデックスを設定\n",
    "def create_index(area_id, item_id, subject_id, element_id, trend_id):\n",
    "    #アイテムごとのユニーク数を数える\n",
    "    area_n = np.unique(area_id).shape[0]\n",
    "    item_n = np.unique(item_id).shape[0]\n",
    "    subject_n = np.unique(subject_id).shape[0]\n",
    "    element_n = np.unique(element_id).shape[0]\n",
    "    trend_n = np.unique(trend_id).shape[0]\n",
    "    \n",
    "    #インデックスを定義\n",
    "    index_area = [i for i in range(area_n)]\n",
    "    index_item = [i for i in range(item_n)]\n",
    "    index_subject = [i for i in range(subject_n)]\n",
    "    index_element = [i for i in range(element_n)]\n",
    "    index_trend = [i for i in range(trend_n)]\n",
    "    for i in range(area_n):\n",
    "        index_area[i] = np.array(np.where(area_id==i)[0], dtype=\"int\")\n",
    "    for i in range(item_n):\n",
    "        index_item[i] = np.array(np.where(item_id==i)[0], dtype=\"int\")\n",
    "    for i in range(subject_n):\n",
    "        index_subject[i] = np.array(np.where(subject_id==i)[0], dtype=\"int\")\n",
    "    for i in range(element_n):\n",
    "        index_element[i] = np.array(np.where(element_id==i)[0], dtype=\"int\")\n",
    "    for i in range(trend_n):\n",
    "        index_trend[i] = np.array(np.where(trend_id==i)[0], dtype=\"int\")\n",
    "    return index_area, index_item, index_subject, index_element, index_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##文書をテキストとaiepに分離する\n",
    "def df_allocation(target_df):\n",
    "    #ユニークなテキストを抽出\n",
    "    index = np.array(np.where(~target_df[\"key\"].duplicated()==True)[0], dtype=\"int\")\n",
    "    key_id = pd.DataFrame(np.arange(index.shape[0])[:, np.newaxis]).rename(columns={0: \"key_id\"})\n",
    "    text_data = target_df[[\"key\", \"date\", \"headline\", \"text\"]].iloc[index]\n",
    "    text_data.index = np.arange(index.shape[0])\n",
    "    text_data = pd.concat((key_id, text_data), axis=1)\n",
    "\n",
    "    #aiepのデータフレームを作成\n",
    "    aiep_data = target_df[[\"key\", \"date\", \"area\", \"subject\", \"item\", \"element\", \"predicate\", \"trend\", \"tags\", \"model\", \"aiep\"]]\n",
    "    temp_id = pd.merge(aiep_data[[\"key\"]], text_data[[\"key\", \"key_id\"]], on=\"key\", how=\"left\")[[\"key_id\"]]\n",
    "    aiep_data = pd.concat((temp_id, aiep_data), axis=1)\n",
    "    return text_data, aiep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##パラグラフ単位のテキストを結合\n",
    "def paragraph_text(text_data):\n",
    "    #データの読み込み\n",
    "    pf2010 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2010.csv\")\n",
    "    pf2010 = pf2010[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2011 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2011.csv\")\n",
    "    pf2011 = pf2011[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2012 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2012.csv\")\n",
    "    pf2012 = pf2012[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2013 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2013.csv\")\n",
    "    pf2013 = pf2013[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2014 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2014.csv\")\n",
    "    pf2014 = pf2014[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2015 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2015.csv\")\n",
    "    pf2015 = pf2015[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2016 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2016.csv\")\n",
    "    pf2016 = pf2016[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2017 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2017.csv\")\n",
    "    pf2017 = pf2017[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf2018 = pd.read_csv(\"D:/Statistics/data/DJ_news_data/DJNML/csv_paragraph/DJNWS_2018.csv\")\n",
    "    pf2018 = pf2018[[\"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    pf = pd.concat((pf2010, pf2011, pf2012, pf2013, pf2014, pf2015, pf2016, pf2017, pf2018), axis=0)\n",
    "    pf.index = np.arange(pf.shape[0])\n",
    "\n",
    "    #ターゲットのテキストを抽出\n",
    "    key_id = np.array(pd.merge(pf[[\"key\"]], text_data[[\"key\", \"key_id\"]], on=\"key\", how=\"left\")[[\"key_id\"]]).reshape(-1)\n",
    "    index_target = np.array(np.where(np.isnan(key_id)==False)[0], dtype=\"int\")\n",
    "    target_pf = pf.iloc[index_target]\n",
    "    target_pf[\"key_id\"] = np.array(key_id[index_target], dtype=\"int\")\n",
    "    target_pf = target_pf[[\"key_id\", \"key\", \"date_jst\", \"type\", \"p_num\", \"text\"]]\n",
    "    target_pf.index = np.arange(target_pf.shape[0])\n",
    "    return target_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sana\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "C:\\Users\\sana\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "##データ解析対象のテキストの抽出とクレンジング\n",
    "#データの読み込み\n",
    "remove_item = np.array([\"原油\", \"石油\", \"米国債\", \"米ドル\", \"ユーロ\", \"日本円\", \"日本国債\", \"株式\"])\n",
    "remove_element = np.array([\"政策金利\", \"金利\", \"株式市場\", \"米国株\", \"株価\", \"利回り\", \"経済\"])\n",
    "read_data = data_input(1, 2, remove_item, remove_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sana\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#データの加工とデータの絞り込み\n",
    "target_df = data_preprocess(read_data)\n",
    "res_delete_news = delete_news(target_df)\n",
    "target_df, area_df, area_id, item_df, item_id, subject_df, subject_id, element_df, element_id, trend_df, trend_id = res_delete_news\n",
    "del res_delete_news\n",
    "\n",
    "#テキスト本文とaiepをそれぞれデータフレーム化する\n",
    "res_correspond_data = correspond_data(target_df, area_id, item_id, subject_id, element_id, trend_id)\n",
    "target_df, area_id, item_id, subject_id, element_id, trend_id, df_date = res_correspond_data\n",
    "del res_correspond_data\n",
    "index_area, index_item, index_subject, index_element, index_trend = create_index(area_id, item_id, subject_id, element_id, trend_id)\n",
    "text_data, aiep_data = df_allocation(target_df)\n",
    "copy_data1 = text_data.copy(); copy_data = aiep_data.copy()   #バックアップ\n",
    "target_pf = paragraph_text(text_data)   #パラグラフ単位のテキストと結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "####テキストの形態素解析の実行####\n",
    "##keyのインデックスとidを作成する関数\n",
    "def make_index(df):\n",
    "    #データの準備\n",
    "    key = np.array(df[[\"key_id\"]]).reshape(-1)\n",
    "    unique_key = np.unique(key)\n",
    "    N = key.shape[0]\n",
    "    n = unique_key.shape[0]\n",
    "    a = pd.DataFrame({\"key\": key})\n",
    "    b = pd.DataFrame({\"no\": np.arange(unique_key.shape[0]), \"key\": unique_key})\n",
    "    temporal_data = pd.merge(a, b , on=\"key\", how=\"left\")\n",
    "    temporal_id = np.array(temporal_data[[\"no\"]]).reshape(-1)\n",
    "\n",
    "    #オブジェクトの格納用配列\n",
    "    index_key = [i for i in range(n)]\n",
    "    n_key = np.repeat(0, n)\n",
    "    first_key = np.repeat(0, n)\n",
    "\n",
    "    #keyのインデックスを作成\n",
    "    for i in range(n):\n",
    "        index_key[i] = np.array(np.where(temporal_id==i)[0], dtype=\"int\")\n",
    "        n_key[i] = index_key[i].shape[0]\n",
    "        first_key[i] = index_key[i][0]\n",
    "    return key, unique_key, temporal_id, N, n, index_key, n_key, first_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##textの形態素解析の実行と結果の整形する関数\n",
    "def mecab_text(df_part, get_class):\n",
    "    #Mecabの設定\n",
    "    m = MeCab.Tagger()\n",
    "\n",
    "    #データの設定\n",
    "    n = df_part.shape[0]\n",
    "    text_vec = np.array(df_part[[\"text\"]]).reshape(-1)\n",
    "    text_list = [i for i in range(n)]\n",
    "    id_list = [i for i in range(n)]\n",
    "    index_list = [i for i in range(n)]\n",
    "    w = np.repeat(0, n)\n",
    "    max_n = 0\n",
    "\n",
    "    #文書ごとに形態素解析結果をリストに格納\n",
    "    for i in range(n):\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        #形態素解析を実行\n",
    "        n_text = neologdn.normalize(text_vec[i])\n",
    "        res = m.parse(n_text)\n",
    "\n",
    "        #結果を整形\n",
    "        parsed_text = res.split('\\n')\n",
    "        parsed_results = pd.Series(parsed_text).str.split('\\t|,').tolist()\n",
    "        temp_df = pd.DataFrame.from_records(parsed_results[0:len(parsed_results)-2])\n",
    "        temp_text = np.array(temp_df)[:, np.array([0, 7, 1, 2])]\n",
    "        index_get = np.where(np.in1d(temp_text[:, 2], get_class))[0]\n",
    "        if index_get.shape[0]==0:\n",
    "            continue\n",
    "        text_list[i] = temp_text[index_get, ]\n",
    "\n",
    "        #IDを格納\n",
    "        w[i] = text_list[i].shape[0]\n",
    "        id_list[i] = np.repeat(i, w[i])\n",
    "        index_list[i] = np.arange(w[i]) + max_n\n",
    "        max_n = np.max(index_list[i]) + 1\n",
    "    return text_list, id_list, index_list, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##形態素結果データフレームを作成\n",
    "def create_word_df(text_list, id_list, index_list, w, target_pf):\n",
    "    #データの設定\n",
    "    d = len(index_list)\n",
    "    f = np.max(index_list[d-1]) + 1\n",
    "    text = np.zeros((f, 4), dtype=\"object\")\n",
    "    d_id = np.repeat(0, f)\n",
    "\n",
    "    #単語配列と単語IDを作成\n",
    "    for i in range(d):\n",
    "        text[index_list[i], ] = text_list[i]\n",
    "        d_id[index_list[i], ] = id_list[i]\n",
    "    index_genkei = np.array(np.where((text[:, 0]!=text[:, 1]) & (text[:, 1]!=\"*\"))[0], dtype=\"int\")\n",
    "    text[index_genkei, 0] = text[index_genkei, 1]\n",
    "    text = text[:, np.array([0, 2, 3])]\n",
    "\n",
    "    #key_idを単語単位に拡張する\n",
    "    index_w = np.where(w > 0)[0]\n",
    "    key_id = np.repeat(np.array(target_pf[\"key_id\"].iloc[index_w]), w[index_w])\n",
    "\n",
    "    #データフレームを作成\n",
    "    word_df = pd.DataFrame({\"key\": key_id, \"id\": d_id, \"word\": text[:, 0], \"word_class1\": text[:, 1], \"word_class2\": text[:, 2]})\n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##単語の名寄せを行う\n",
    "def replace_word(word_df):\n",
    "    #データの読み込み\n",
    "    area_dic = pd.read_csv(\"D:/Statistics/data/dic/area_pattern_freq.csv\", encoding=\"Shift-Jis\")\n",
    "    item_dic = pd.read_csv(\"D:/Statistics/data/dic/item_pattern_freq.csv\", encoding=\"Shift-Jis\")\n",
    "\n",
    "    #単語の置き換え\n",
    "    temp_df = word_df.copy()\n",
    "    replace_word = pd.merge(temp_df, item_dic[[\"input\", \"output\"]], left_on=\"word\", right_on=\"input\", how=\"left\")[\"output\"]\n",
    "    index_target = np.array(np.where(pd.isna(replace_word)==False)[0], dtype=\"int\")\n",
    "    temp_df[\"word\"].iloc[index_target] = replace_word.iloc[index_target]\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##不要な単語を削除\n",
    "def delete_word(word_df, trunc_word):\n",
    "    ##低頻度語を削除\n",
    "    #単語頻度を集計\n",
    "    word_freq = word_df[\"word\"].value_counts()\n",
    "    word_freq = pd.DataFrame({\"word\": np.array(word_freq.index), \"freq\": word_freq})\n",
    "    word_freq.index = np.arange(word_freq.shape[0])\n",
    "    \n",
    "    #低頻度語を特定し、データフレームから削除\n",
    "    index_target = np.array(np.where(word_freq[\"freq\"] >= trunc_word)[0], dtype=\"int\")\n",
    "    target_word = np.array(word_freq[\"word\"].iloc[index_target])\n",
    "    j = np.array(pd.merge(word_df, pd.DataFrame({\"word\": target_word, \"no\": 1}), on=\"word\", how=\"left\")[\"no\"])\n",
    "    word_df = word_df.iloc[np.where(~np.isnan(j))[0]]\n",
    "    word_df.index = np.arange(word_df.shape[0])\n",
    "\n",
    "    ##ストップワードを削除\n",
    "    #辞書の読み込み\n",
    "    stopword = pd.read_table(\"D:/Statistics/data/dic/stopword_jp.txt\", header=None)\n",
    "    stopword = stopword.rename(columns={0: \"word\"})\n",
    "    stopword[\"flag\"] = 1\n",
    "\n",
    "    #データフレームから単語を削除\n",
    "    j = np.array(pd.merge(word_df, stopword, on=\"word\", how=\"left\")[\"flag\"])\n",
    "    index_target = np.array(np.where(np.isnan(j))[0], dtype=\"int\")\n",
    "    word_df = word_df.iloc[index_target]\n",
    "    word_df.index = np.arange(word_df.shape[0])\n",
    "    \n",
    "    ##隣接する重複を削除する\n",
    "    index1 = np.arange(word_df.shape[0])[:word_df.shape[0]-1]\n",
    "    index2 = np.arange(word_df.shape[0])[np.arange(1, word_df.shape[0])]\n",
    "    index_dup1 = np.array(word_df[\"word\"].iloc[index1])==np.array(word_df[\"word\"].iloc[index2])\n",
    "    index_dup1 = np.array(index_dup1, dtype=\"int\")\n",
    "    index_dup2 = np.array(word_df[\"id\"].iloc[index1])==np.array(word_df[\"id\"].iloc[index2])\n",
    "    index_dup2 = np.array(index_dup2, dtype=\"int\")\n",
    "    index_dup = np.array(np.where((index_dup1+index_dup2) < 2)[0], dtype=\"int\")\n",
    "    word_df = word_df.iloc[index_dup]\n",
    "    word_df.index = np.arange(word_df.shape[0])\n",
    "    \n",
    "    #数値を削除\n",
    "    index = np.array(np.where(word_df[\"word_class2\"]!=\"数\")[0])\n",
    "    word_df = word_df.iloc[index]\n",
    "    word_df.index = np.arange(word_df.shape[0])\n",
    "    \n",
    "    ##単語頻度の集計と低頻度語の処理\n",
    "    #単語頻度を集計\n",
    "    word_freq = word_df[\"word\"].value_counts()\n",
    "    word_freq = pd.DataFrame({\"word\": np.array(word_freq.index), \"freq\": word_freq})\n",
    "    word_freq.index = np.arange(word_freq.shape[0])\n",
    "    word_freq.to_csv(\"D:/Statistics/data/word_freq.csv\", index=None)\n",
    "        \n",
    "    #一定上の頻度ごとの語彙数と総単語数を集計\n",
    "    freq = np.array([1, 3, 5, 10, 20, 30, 50, 75, 100])\n",
    "    w_agg = np.zeros((len(freq), 3), dtype=\"int\")\n",
    "    for j in range(len(freq)):\n",
    "        index = np.where(word_freq[\"freq\"] >= freq[j])[0]\n",
    "        target = word_freq[\"freq\"].iloc[index]\n",
    "        w_agg[j, ] = np.array([freq[j], target.shape[0], np.sum(target)], dtype=\"int\")\n",
    "    return word_df, word_freq, w_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IDを割り当て直す\n",
    "def setting_id(word_df):\n",
    "    #sentence idを設定\n",
    "    sentence_id = np.unique(word_df[\"id\"])\n",
    "    s = sentence_id.shape[0]\n",
    "    word_df = word_df.rename(columns={\"id\": \"original_id\"})\n",
    "    new_id = pd.DataFrame({\"original_id\": sentence_id, \"new_id\": np.arange(s)})\n",
    "    word_df = pd.merge(word_df, new_id, on=\"original_id\", how=\"left\")\n",
    "\n",
    "    #word idを設定\n",
    "    word = pd.unique(word_df[\"word\"])\n",
    "    v = word.shape[0]\n",
    "    word_id = pd.DataFrame({\"word\": word, \"word_id\": np.arange(v)})\n",
    "    word_df = pd.merge(word_df, word_id, on=\"word\", how=\"left\")\n",
    "    word_df = word_df[[\"key\", \"original_id\", \"new_id\", \"word\", \"word_id\", \"word_class1\", \"word_class2\"]]\n",
    "    \n",
    "    #idをベクトル化\n",
    "    key = np.array(word_df[\"key\"])\n",
    "    sentence = np.array(word_df[\"new_id\"])\n",
    "    wd = np.array(word_df[\"word_id\"])\n",
    "    return word_df, key, sentence, wd, new_id, word_id, s, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##インデックスの作成\n",
    "def setting_index(key, sentence, wd, n, s, v):\n",
    "    #頻度を集計\n",
    "    sentence_freq = pd.Series(word_df[\"new_id\"]).value_counts()\n",
    "    sentence_freq = np.array(sentence_freq.iloc[np.argsort(np.array(sentence_freq.index))], dtype=\"int\")\n",
    "    key_freq = pd.Series(word_df[\"key\"]).value_counts()\n",
    "    key_freq = np.array(key_freq.iloc[np.argsort(np.array(key_freq.index))], dtype=\"int\")\n",
    "\n",
    "    #インデックスの格納用\n",
    "    key_list = [i for i in range(n)]\n",
    "    sentence_list = [i for i in range(s)]\n",
    "    wd_list = [i for i in range(v)]\n",
    "\n",
    "    #インデックスをリストに格納\n",
    "    max_n = 0\n",
    "    for i in range(n):\n",
    "        key_list[i] = np.arange(key_freq[i]) + max_n\n",
    "        max_n = np.max(key_list[i]) + 1\n",
    "    max_n = 0\n",
    "    for i in range(s):\n",
    "        sentence_list[i] = np.arange(sentence_freq[i]) + max_n\n",
    "        max_n = np.max(sentence_list[i]) + 1\n",
    "    for i in range(v):\n",
    "        wd_list[i] = np.array(np.where(wd==i)[0], dtype=\"int\")\n",
    "    return key_list, sentence_list, wd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##テキストのクレンジング\n",
    "#文字列の正規化や記号の削除を行う\n",
    "target_pf[[\"text\"]] = target_pf[\"text\"].str.lower()\n",
    "target_pf[[\"text\"]] = target_pf[\"text\"].str.normalize(\"NFKC\")\n",
    "target_pf[[\"text\"]] = target_pf[\"text\"].str.replace(\",\", \"\")\n",
    "target_pf[[\"text\"]] = target_pf[\"text\"].str.replace(\"*\", \"\")\n",
    "target_pf[[\"text\"]] = target_pf[\"text\"].str.replace(\"[0-9]+\", \"0\")\n",
    "\n",
    "#データのkeyのインデックスとidを作成\n",
    "res = make_index(target_pf)\n",
    "key = res[0]\n",
    "unique_key = res[1]\n",
    "temporal_id = res[2]\n",
    "N = res[3]\n",
    "n = res[4]\n",
    "index_key = res[5]\n",
    "n_key = res[6]\n",
    "first_key = res[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n"
     ]
    }
   ],
   "source": [
    "##形態素解析を実行\n",
    "#文書ごとに形態素解析\n",
    "get_class = np.array([\"名詞\", \"動詞\", \"形容詞\"])\n",
    "res = mecab_text(target_pf, get_class)\n",
    "text_list = res[0]\n",
    "id_list = res[1]\n",
    "index_list = res[2]\n",
    "w = res[3]\n",
    "\n",
    "#形態素結果のデータフレームを作成\n",
    "word_df = create_word_df(text_list, id_list, index_list, w, target_pf)\n",
    "word_df.to_csv(\"D:/Statistics/data/word_df_backup.csv\", index=None)\n",
    "del res, text_list, id_list, index_list, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sana\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "##不要な単語の削除とidの設定\n",
    "#単語の名寄せ\n",
    "word_df = pd.read_csv(\"D:/Statistics/data/word_df_backup.csv\", index_col=None)\n",
    "word_df = replace_word(word_df)   \n",
    "\n",
    "#低頻度語とストップワード削除\n",
    "res = delete_word(word_df, 10)\n",
    "word_df = res[0]\n",
    "del res\n",
    "\n",
    "#センテンスと単語のidを設定\n",
    "res = setting_id(word_df)\n",
    "word_df = res[0]\n",
    "key = res[1]\n",
    "sentence = res[2]\n",
    "wd = res[3]\n",
    "co_sentence = res[4]\n",
    "co_word = res[5]\n",
    "s = res[6]\n",
    "v = res[7]\n",
    "del res\n",
    "\n",
    "#インデックスの作成\n",
    "res = setting_index(key, sentence, wd, n, s, v)\n",
    "key_list = res[0]\n",
    "sentence_list = res[1]\n",
    "wd_list = res[2]\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Joint Hierarchical Structured Topic Modelを推定####\n",
    "##パラメータを推定するための関数\n",
    "#多項分布の乱数を生成する関数\n",
    "def rmnom(pr, n, k, no, pattern):\n",
    "    z_id = np.argmax((np.cumsum(pr, axis=1) >= np.random.uniform(0, 1, n)[:, np.newaxis]), axis=1)\n",
    "    if pattern==1:\n",
    "        Z = sparse.coo_matrix((np.repeat(1, n), (no, np.array(z_id))), shape=(n, k))   #スパース行列の設定\n",
    "        return z_id, Z\n",
    "    return z_id\n",
    "\n",
    "#トピック尤度と負担率を計算する関数\n",
    "def LLho(theta, phi, d_id, wd, f, k):\n",
    "    Lho = theta[d_id, ] * (phi.T)[wd, ]\n",
    "    topic_rate = Lho / np.sum(Lho, axis=1)[:, np.newaxis]\n",
    "    return Lho, topic_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##アルゴリズムの設定\n",
    "k = 50\n",
    "R = 500\n",
    "keep = 1\n",
    "burnin = int(250/keep)\n",
    "iter = 0\n",
    "disp = 10\n",
    "e1 = 0.005\n",
    "e2 = 0.025\n",
    "e3 = 0.01\n",
    "L1 = 3\n",
    "L2 = 1\n",
    "k_vec = np.repeat(1, k)\n",
    "f = word_df.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##名詞と動詞を分離する\n",
    "#新しい単語idを作成\n",
    "index_noun = np.array(np.where(word_df[\"word_class1\"]==\"名詞\")[0], dtype=\"int\")\n",
    "index_verb = np.array(np.where(word_df[\"word_class1\"]!=\"名詞\")[0], dtype=\"int\")\n",
    "wd_df1 = wd[index_noun]\n",
    "wd_df2 = wd[index_verb]\n",
    "unique_wd1 = np.unique(wd_df1); v1 = unique_wd1.shape[0]\n",
    "unique_wd2 = np.unique(wd_df2); v2 = unique_wd2.shape[0]\n",
    "wd_df1 = pd.merge(pd.DataFrame({\"wd\": wd_df1}), pd.DataFrame({\"wd\": unique_wd1, \"wd1\": np.arange(v1)}), on=\"wd\", how=\"left\")\n",
    "wd_df2 = pd.merge(pd.DataFrame({\"wd\": wd_df2}), pd.DataFrame({\"wd\": unique_wd2, \"wd2\": np.arange(v2)}), on=\"wd\", how=\"left\")\n",
    "wd_df1[\"word\"] = np.array(word_df[\"word\"].iloc[index_noun])\n",
    "wd_df2[\"word\"] = np.array(word_df[\"word\"].iloc[index_verb])\n",
    "wd1 = np.array(wd_df1[\"wd1\"], dtype=\"int\")\n",
    "wd2 = np.array(wd_df2[\"wd2\"], dtype=\"int\")\n",
    "\n",
    "#sentence idを分離する\n",
    "sentence1 = sentence[index_noun]\n",
    "sentence2 = sentence[index_verb]\n",
    "\n",
    "#インデックスを作成\n",
    "wd_list1 = [i for i in range(v1)]; wd_dt1 = [i for i in range(v1)]\n",
    "wd_list2 = [i for i in range(v2)]; wd_dt2 = [i for i in range(v2)]\n",
    "for i in range(v1):\n",
    "    wd_list1[i] = np.array(np.where(wd1==i)[0], dtype=\"int\")\n",
    "    wd_dt1[i] = np.repeat(1, wd_list1[i].shape[0])\n",
    "for i in range(v2):\n",
    "    wd_list2[i] = np.array(np.where(wd2==i)[0], dtype=\"int\")\n",
    "    wd_dt2[i] = np.repeat(1, wd_list2[i].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##文書とタグを対応付ける(itemとelementを統合)\n",
    "#aiep idを作成\n",
    "index_item = np.array(np.where(pd.isna(aiep_data[\"item\"])==False)[0], dtype=\"int\")\n",
    "index_element = np.array(np.where(pd.isna(aiep_data[\"element\"])==False)[0], dtype=\"int\")\n",
    "joint_id = np.array(aiep_data[\"key_id\"].iloc[np.append(index_item, index_element)], dtype=\"int\")\n",
    "sortlist = np.array(np.argsort(joint_id), dtype=\"int\")\n",
    "joint_id = joint_id[sortlist]\n",
    "aiep = np.append(aiep_data[\"item\"].iloc[index_item], aiep_data[\"element\"].iloc[index_element])[sortlist]\n",
    "unique_aiep = pd.unique(aiep); aiep_n = unique_aiep.shape[0]\n",
    "aiep_df = pd.merge(pd.DataFrame({\"aiep\": aiep}), pd.DataFrame({\"aiep\": unique_aiep, \"id\": np.arange(aiep_n)}), on=\"aiep\", how=\"left\")\n",
    "aiep_id = np.array(aiep_df[\"id\"])\n",
    "\n",
    "#インデックスを作成\n",
    "aiep_list = [i for i in range(aiep_n)]\n",
    "aiep_dt = [i for i in range(aiep_n)]\n",
    "for i in range(aiep_n):\n",
    "    aiep_list[i] = np.array(np.where(aiep_id==i)[0], dtype=\"int\")\n",
    "    aiep_dt[i] = np.repeat(1, aiep_list[i].shape[0])\n",
    "g = aiep_id.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##文書とタグを対応付ける(itemとelementは別々)\n",
    "#aiep idを作成\n",
    "index_item = np.array(np.where(pd.isna(aiep_data[\"item\"])==False)[0], dtype=\"int\")\n",
    "index_element = np.array(np.where(pd.isna(aiep_data[\"element\"])==False)[0], dtype=\"int\")\n",
    "joint_id1 = np.array(aiep_data[\"key_id\"].iloc[index_item], dtype=\"int\")\n",
    "joint_id2 = np.array(aiep_data[\"key_id\"].iloc[index_element], dtype=\"int\")\n",
    "aiep_item = np.array(aiep_data[\"item\"].iloc[index_item])\n",
    "aiep_element = np.array(aiep_data[\"element\"].iloc[index_element])\n",
    "unique_item = pd.unique(aiep_item); unique_element = pd.unique(aiep_element)\n",
    "item_n = unique_item.shape[0]; element_n = unique_element.shape[0]\n",
    "aiep_df1 = pd.merge(pd.DataFrame({\"aiep\": aiep_item}), \n",
    "                    pd.DataFrame({\"aiep\": unique_item, \"id\": np.arange(item_n)}), on=\"aiep\", how=\"left\")\n",
    "aiep_df2 = pd.merge(pd.DataFrame({\"aiep\": aiep_element}), \n",
    "                    pd.DataFrame({\"aiep\": unique_element, \"id\": np.arange(element_n)}), on=\"aiep\", how=\"left\")\n",
    "aiep_id1 = np.array(aiep_df1[\"id\"]); aiep_id2 = np.array(aiep_df2[\"id\"])\n",
    "\n",
    "#インデックスを作成\n",
    "aiep_list1 = [i for i in range(item_n)]; aiep_list2 = [i for i in range(element_n)]\n",
    "aiep_dt1 = [i for i in range(item_n)]; aiep_dt2 = [i for i in range(element_n)]\n",
    "for i in range(item_n):\n",
    "    aiep_list1[i] = np.array(np.where(aiep_id1==i)[0], dtype=\"int\")\n",
    "    aiep_dt1[i] = np.repeat(1, aiep_list1[i].shape[0])\n",
    "for i in range(element_n):\n",
    "    aiep_list2[i] = np.array(np.where(aiep_id2==i)[0], dtype=\"int\")\n",
    "    aiep_dt2[i] = np.repeat(1, aiep_list2[i].shape[0])  \n",
    "g1 = aiep_id1.shape[0]\n",
    "g2 = aiep_id2.shape[0]\n",
    "g = g1 + g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#和を計算するためのnベクトルを定義\n",
    "noun_vec = np.array(word_df[\"word_class1\"]==\"名詞\", dtype=\"int\")\n",
    "key_dt = [i for i in range(n)]\n",
    "noun_dt = [i for i in range(n)]\n",
    "sentence_dt = [i for i in range(s)]\n",
    "noun_n = np.repeat(0, n)\n",
    "sentence_n = np.repeat(0, s)\n",
    "for i in range(n):\n",
    "    key_dt[i] = np.repeat(1, key_list[i].shape[0])\n",
    "    noun_dt[i] = noun_vec[key_list[i]]\n",
    "    noun_n[i] = np.sum(noun_dt[i])\n",
    "for i in range(s):\n",
    "    sentence_dt[i] = np.repeat(1, sentence_list[i].shape[0])\n",
    "    sentence_n[i] = sentence_dt[i].shape[0]    \n",
    "noun_n = noun_n[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文書とセンテンスのidを対応付ける\n",
    "index_dup = np.array(np.where(word_df[[\"key\", \"new_id\"]].duplicated()==False)[0], dtype=\"int\")\n",
    "correspond_id = np.array(word_df[\"key\"].iloc[index_dup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##事前分布の設定\n",
    "alpha01 = 0.25\n",
    "beta01 = 0.1\n",
    "beta02 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##初期値の設定\n",
    "#ハイパーパラメータの初期値\n",
    "delta = np.array(np.full((s, k), 0.25), dtype=\"float32\")\n",
    "delta_n = sentence_n.copy(); delta_n[delta_n < 5] = 5\n",
    "er = 0.001\n",
    "\n",
    "#トピック分布の初期値\n",
    "theta_d = np.array(np.random.dirichlet(np.repeat(5.0, k), n), dtype=\"float32\")\n",
    "theta_s = np.array(np.random.dirichlet(np.repeat(5.0, k), s), dtype=\"float32\")\n",
    "\n",
    "#単語分布の初期値\n",
    "phi = np.array(np.random.dirichlet(np.repeat(5.0, v), k), dtype=\"float32\")\n",
    "phi1 = np.array(np.random.dirichlet(np.repeat(5.0, v1), k), dtype=\"float32\")\n",
    "phi2 = np.array(np.random.dirichlet(np.repeat(5.0, v2), k), dtype=\"float32\")\n",
    "omega1 = np.array(np.random.dirichlet(np.repeat(5.0, item_n), k), dtype=\"float32\")\n",
    "omega2 = np.array(np.random.dirichlet(np.repeat(5.0, element_n), k), dtype=\"float32\")\n",
    "\n",
    "#アルゴリズム推定用配列\n",
    "Lho1 = np.zeros((f, k), dtype=\"float32\")\n",
    "Lho21 = np.zeros((g1, k), dtype=\"float32\")\n",
    "Lho22 = np.zeros((g2, k), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##パラメータの格納用配列\n",
    "#トピックの格納用配列\n",
    "SEG1 = np.zeros((f, k), dtype=\"int16\")\n",
    "SEG21 = np.zeros((g1, k), dtype=\"int16\")\n",
    "SEG22 = np.zeros((g2, k), dtype=\"int16\")\n",
    "\n",
    "#パラメータの格納用配列\n",
    "THETA_D = np.zeros((n, k), dtype=\"float32\")\n",
    "THETA_S = np.zeros((s, k), dtype=\"float32\")\n",
    "PHI1 = np.zeros((k, v1), dtype=\"float32\")\n",
    "PHI2 = np.zeros((k, v2), dtype=\"float32\")\n",
    "OMEGA1 = np.zeros((k, item_n), dtype=\"float32\")\n",
    "OMEGA2 = np.zeros((k, element_n), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-58485881.8, -57435252.0, -434232.9, -616400.9]\n",
      "10\n",
      "[-58474526.2, -57423904.0, -434687.6, -615934.6]\n",
      "20\n",
      "[-58464869.5, -57413856.0, -434962.4, -616051.2]\n",
      "30\n",
      "[-58462197.7, -57410696.0, -435187.1, -616318.6]\n",
      "40\n",
      "[-58451161.2, -57399830.0, -435379.9, -615949.3]\n",
      "50\n",
      "[-58445364.0, -57393184.0, -435904.1, -616275.9]\n",
      "60\n",
      "[-58436630.2, -57384650.0, -436300.1, -615682.2]\n",
      "70\n",
      "[-58430367.5, -57378376.0, -436417.5, -615574.0]\n",
      "80\n",
      "[-58423310.0, -57370732.0, -436879.9, -615698.1]\n",
      "90\n",
      "[-58411978.7, -57359604.0, -437023.4, -615355.3]\n",
      "100\n",
      "[-58412000.0, -57359320.0, -437146.2, -615529.9]\n",
      "110\n",
      "[-58409343.8, -57357068.0, -436947.0, -615328.8]\n",
      "120\n",
      "[-58403195.6, -57350296.0, -437282.8, -615616.9]\n",
      "130\n",
      "[-58406197.6, -57353250.0, -437188.4, -615761.3]\n",
      "140\n",
      "[-58401722.2, -57349184.0, -437118.3, -615419.9]\n",
      "150\n",
      "[-58401240.6, -57348344.0, -437419.8, -615472.8]\n",
      "160\n",
      "[-58397962.9, -57344972.0, -437357.1, -615633.7]\n",
      "170\n",
      "[-58396375.1, -57343884.0, -436998.7, -615488.5]\n",
      "180\n",
      "[-58397113.8, -57344504.0, -437203.7, -615402.1]\n",
      "190\n",
      "[-58397686.0, -57345064.0, -437287.1, -615338.9]\n",
      "200\n",
      "[-58395211.0, -57342630.0, -437264.0, -615315.0]\n",
      "210\n",
      "[-58395262.9, -57342292.0, -437277.2, -615693.7]\n",
      "220\n",
      "[-58392891.3, -57340040.0, -437410.5, -615440.8]\n"
     ]
    }
   ],
   "source": [
    "####パラメータをサンプリング####\n",
    "for rp in range(R):\n",
    "\n",
    "    ##単語トピックを生成\n",
    "    #トピック選択確率を定義 \n",
    "    Lho1[index_noun, ] = theta_s[sentence1, ] * (phi1.T)[wd1, ]\n",
    "    Lho1[index_verb, ] = theta_s[sentence2, ] * (phi2.T)[wd2, ]\n",
    "    topic_prob = Lho1 / np.dot(Lho1, k_vec)[:, np.newaxis]\n",
    "\n",
    "    #多項分布からトピックを生成\n",
    "    Zi = np.array(rmnom(topic_prob, f, k, np.arange(f), 1)[1].todense(), dtype=\"int8\")\n",
    "\n",
    "    ##ディリクレ分布からパラメータをサンプリング\n",
    "    #グローバルトピック分布をサンプリング\n",
    "    y = np.zeros((n, k), dtype=\"int\")\n",
    "    for i in range(n):\n",
    "        z = Zi[key_list[i], ]\n",
    "        y[i, ] = np.dot(noun_dt[i], z)\n",
    "        x = np.dot(key_dt[i], z) + alpha01\n",
    "        theta_d[i, ] = np.random.dirichlet(x, 1).reshape(-1)\n",
    "\n",
    "    #ハイパーパラメータを更新\n",
    "    delta_vec = np.dot(delta, k_vec)\n",
    "    delta1 = psi(theta_d[correspond_id, ] + delta) - psi(delta)\n",
    "    delta2 = (psi(delta_n + delta_vec) - psi(delta_vec))[:, np.newaxis]\n",
    "    new_delta = delta * (delta1 / delta2) + er\n",
    "\n",
    "    #ローカルトピック分布をサンプリング\n",
    "    for i in range(s):\n",
    "        x = np.dot(sentence_dt[i], Zi[sentence_list[i], ]) + new_delta[i, ]\n",
    "        theta_s[i, ] = np.random.dirichlet(x, 1)\n",
    "    delta = new_delta.copy()\n",
    "\n",
    "    #単語分布サンプリング\n",
    "    Zi1 = Zi[index_noun, ]; Zi2 = Zi[index_verb, ]\n",
    "    x1 = np.zeros((k, v1)); x2 = np.zeros((k, v2))\n",
    "    for i in range(v1):\n",
    "        x1[:, i] = np.dot(wd_dt1[i], Zi1[wd_list1[i], ]) \n",
    "        if i < v2:\n",
    "            x2[:, i] = np.dot(wd_dt2[i], Zi2[wd_list2[i], ]) \n",
    "    for j in range(k):\n",
    "        phi1[j, ] = np.random.dirichlet(x1[j, ] + beta01, 1)\n",
    "        phi2[j, ] = np.random.dirichlet(x2[j, ] + beta01, 1)\n",
    "    del Zi1, Zi2\n",
    "\n",
    "    ##タグトピックを生成\n",
    "    #トピック選択確率を定義\n",
    "    theta_mu = y / noun_n   #グローバルトピックの経験分布\n",
    "    Lho21 = theta_mu[joint_id1, ] * (omega1.T)[aiep_id1, ]\n",
    "    Lho22 = theta_mu[joint_id2, ] * (omega2.T)[aiep_id2, ]\n",
    "    topic_prob1 = Lho21 / np.dot(Lho21, k_vec)[:, np.newaxis]\n",
    "    topic_prob2 = Lho22 / np.dot(Lho22, k_vec)[:, np.newaxis]\n",
    "\n",
    "    #多項分布からトピックを生成\n",
    "    Si1 = np.array(rmnom(topic_prob1, g1, k, np.arange(g1), 1)[1].todense(), dtype=\"int8\")\n",
    "    Si2 = np.array(rmnom(topic_prob2, g2, k, np.arange(g2), 1)[1].todense(), dtype=\"int8\")\n",
    "\n",
    "    ##ディリクレ分布からパラメータをサンプリング\n",
    "    #タグ分布をサンプリング\n",
    "    x1 = np.zeros((k, item_n))\n",
    "    x2 = np.zeros((k, element_n))\n",
    "    for i in range(item_n):\n",
    "        x1[:, i] = np.dot(aiep_dt1[i], Si1[aiep_list1[i], ])\n",
    "        if i < element_n:\n",
    "            x2[:, i] = np.dot(aiep_dt2[i], Si2[aiep_list2[i], ]) \n",
    "    for j in range(k):\n",
    "        omega1[j, ] = np.random.dirichlet(x1[j, ] + beta02, 1)\n",
    "        omega2[j, ] = np.random.dirichlet(x2[j, ] + beta02, 1)\n",
    "\n",
    "\n",
    "    ##サンプリング結果の格納用と表示\n",
    "    if (rp >= burnin) & (rp%keep==0):\n",
    "        SEG1 += Zi\n",
    "        SEG21 += Si1\n",
    "        SEG22 += Si2\n",
    "        THETA_D += theta_d\n",
    "        THETA_S += theta_s\n",
    "        PHI1 += phi1\n",
    "        PHI2 += phi2\n",
    "        OMEGA1 += omega1\n",
    "        OMEGA2 += omega2\n",
    "        \n",
    "    if rp%disp==0: \n",
    "        #対数尤度を更新\n",
    "        LLho1 = np.sum(np.log(np.sum(Lho1, axis=1)))\n",
    "        LLho21 = np.sum(np.log(np.sum(Lho21, axis=1)))\n",
    "        LLho22 = np.sum(np.log(np.sum(Lho22, axis=1)))\n",
    "        LLho = LLho1 + LLho21 + LLho22\n",
    "\n",
    "        #サンプリング結果を表示\n",
    "        print(rp)\n",
    "        print([np.round(LLho, 1), np.round(LLho1, 1), np.round(LLho21, 1), np.round(LLho22, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##結果の確認\n",
    "r = np.arange(burnin, R).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##タグの類似度を出力\n",
    "#phiのサンプリング結果\n",
    "new_word = wd_df1.sort_values(by=\"wd1\")\n",
    "new_word.index = np.arange(new_word.shape[0])\n",
    "word = np.array(new_word[\"word\"].iloc[np.where(new_word[\"wd1\"].duplicated()==False)[0]])\n",
    "word_freq = np.array([np.sum(wd_dt1[i]) for i in range(v1)])\n",
    "phi_sum = word_freq[:, np.newaxis] * (PHI1.T)\n",
    "topic_prob = phi_sum / np.sum(phi_sum, axis=1)[:, np.newaxis]\n",
    "\n",
    "#omegaのサンプリング結果\n",
    "tag_freq = np.array([aiep_list[i].shape[0] for i in range(aiep_n)])\n",
    "omega_sum = tag_freq[:, np.newaxis] * (OMEGA.T)\n",
    "\n",
    "#文書とタグの両方に出てくる単語を抽出\n",
    "j = pd.merge(pd.DataFrame({\"word\": word}), pd.DataFrame({\"word\": unique_aiep, \"flag\": 1}), on=\"word\", how=\"left\")\n",
    "index_target = np.array(np.where(pd.isna(j[\"flag\"])==False)[0], dtype=\"int\")\n",
    "j_target = j.iloc[index_target]; j_target.index = np.arange(j_target.shape[0])\n",
    "phi_target = phi_sum[index_target, ]\n",
    "freq_target = word_freq[index_target, ]\n",
    "\n",
    "#文書全体のトピックをタグに結合\n",
    "j_freq = tag_freq.copy()\n",
    "j_index = np.array(pd.merge(pd.DataFrame({\"word\": j_target[\"word\"]}), \n",
    "                   pd.DataFrame({\"word\": unique_aiep, \"no\": np.arange(unique_aiep.shape[0])}), on=\"word\", how=\"left\")[\"no\"])\n",
    "omega_sum[j_index, ] = phi_target + omega_sum[j_index, ]\n",
    "j_freq[j_index] = freq_target + j_freq[j_index]\n",
    "\n",
    "#結合分布を出力\n",
    "aux_prob = omega_sum / np.sum(omega_sum, axis=1)[:, np.newaxis]\n",
    "res_omega = pd.concat((pd.DataFrame({\"id\": np.arange(aiep_n), \"aiep\": unique_aiep}), pd.DataFrame(aux_prob)), axis=1)\n",
    "res_omega.to_csv(\"D:/Statistics/data/res_omega.csv\", index=None, encoding=\"Shift-Jis\")\n",
    "\n",
    "#コサイン類似度を計算\n",
    "q = np.sqrt(np.sum(np.power(aux_prob, 2), axis=1))[:, np.newaxis]\n",
    "cos = np.dot(aux_prob, aux_prob.T) / np.dot(q, q.T)\n",
    "cos = pd.DataFrame(cos)\n",
    "cos.columns = unique_aiep; cos.index = unique_aiep\n",
    "cos.to_csv(\"D:/Statistics/data/omega_cos.csv\", encoding=\"Shift-Jis\")\n",
    "\n",
    "#タグごとに類似している上位10単語を出力\n",
    "aiep_freq = np.repeat(0, unique_aiep.shape[0])\n",
    "similar_word = np.full((unique_aiep.shape[0], 10), \"\", dtype=\"object\")\n",
    "similar_score = np.zeros((unique_aiep.shape[0], 10), dtype=\"float32\")\n",
    "for i in range(unique_aiep.shape[0]):\n",
    "    aiep_freq[i] = aiep_list[i].shape[0]\n",
    "    a = cos[[unique_aiep[i]]].sort_values(by=unique_aiep[i], ascending=False)\n",
    "    similar_word[i, ] = np.array(a.iloc[1:11].index)\n",
    "    similar_score[i, ] = np.array(a.iloc[1:11]).reshape(-1)\n",
    "similar_out = pd.concat((pd.DataFrame({\"aiep\": unique_aiep, \"freq1\": aiep_freq, \"freq2\": j_freq}),\n",
    "                         pd.DataFrame(similar_word), pd.DataFrame(similar_score)), axis=1)\n",
    "similar_out.to_csv(\"D:/Statistics/data/similar_out.csv\", index=None, encoding=\"Shift-Jis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
